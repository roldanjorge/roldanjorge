<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>N-Gram Language Models | Jorge Roldan</title>
<meta name=keywords content="ngram,language_models,nlp"><meta name=description content="N-gram Language Models
N-Grams
N-gram models are the simplest type of language models. The N-gram term
has two meanings. One meaning refers to a sequence of n words, so a
2-gram, and 3-gram are sequences of 2, and 3 words, respectively. The
second meaning refers to a probabilistic model that estimates the
probability of a word given the n-1 previous words.
We represent a sequence of n words as $w_1 \dots w_n$ or $w_{1:n}$ , and
the join probability of each word in a sequence having a value:
$P(W_1 = w_1, X_2=w_2, X_3=w_3, \dots, X_n = w_n$"><meta name=author content="Jorge Roldan"><link rel=canonical href=http://localhost:1313/posts/2_n_gram_language_models/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/2_n_gram_language_models/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/2_n_gram_language_models/"><meta property="og:site_name" content="Jorge Roldan"><meta property="og:title" content="N-Gram Language Models"><meta property="og:description" content="N-gram Language Models N-Grams N-gram models are the simplest type of language models. The N-gram term has two meanings. One meaning refers to a sequence of n words, so a 2-gram, and 3-gram are sequences of 2, and 3 words, respectively. The second meaning refers to a probabilistic model that estimates the probability of a word given the n-1 previous words.
We represent a sequence of n words as $w_1 \dots w_n$ or $w_{1:n}$ , and the join probability of each word in a sequence having a value: $P(W_1 = w_1, X_2=w_2, X_3=w_3, \dots, X_n = w_n$"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-26T00:00:00+00:00"><meta property="article:modified_time" content="2025-02-26T00:00:00+00:00"><meta property="article:tag" content="Ngram"><meta property="article:tag" content="Language_models"><meta property="article:tag" content="Nlp"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="N-Gram Language Models"><meta name=twitter:description content="N-gram Language Models
N-Grams
N-gram models are the simplest type of language models. The N-gram term
has two meanings. One meaning refers to a sequence of n words, so a
2-gram, and 3-gram are sequences of 2, and 3 words, respectively. The
second meaning refers to a probabilistic model that estimates the
probability of a word given the n-1 previous words.
We represent a sequence of n words as $w_1 \dots w_n$ or $w_{1:n}$ , and
the join probability of each word in a sequence having a value:
$P(W_1 = w_1, X_2=w_2, X_3=w_3, \dots, X_n = w_n$"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"N-Gram Language Models","item":"http://localhost:1313/posts/2_n_gram_language_models/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"N-Gram Language Models","name":"N-Gram Language Models","description":"N-gram Language Models N-Grams N-gram models are the simplest type of language models. The N-gram term has two meanings. One meaning refers to a sequence of n words, so a 2-gram, and 3-gram are sequences of 2, and 3 words, respectively. The second meaning refers to a probabilistic model that estimates the probability of a word given the n-1 previous words.\nWe represent a sequence of n words as $w_1 \\dots w_n$ or $w_{1:n}$ , and the join probability of each word in a sequence having a value: $P(W_1 = w_1, X_2=w_2, X_3=w_3, \\dots, X_n = w_n$\n","keywords":["ngram","language_models","nlp"],"articleBody":"N-gram Language Models N-Grams N-gram models are the simplest type of language models. The N-gram term has two meanings. One meaning refers to a sequence of n words, so a 2-gram, and 3-gram are sequences of 2, and 3 words, respectively. The second meaning refers to a probabilistic model that estimates the probability of a word given the n-1 previous words.\nWe represent a sequence of n words as $w_1 \\dots w_n$ or $w_{1:n}$ , and the join probability of each word in a sequence having a value: $P(W_1 = w_1, X_2=w_2, X_3=w_3, \\dots, X_n = w_n$\nTo compute the probability of $P(w_1, w_2, \\dots, w_n)$ we use the chain rule of probability\n$$P(X_1, \\dots, X_n) = P(X_1) P(X_2 | X_1) P(X_3 | X_{1:2} \\dots P(X_n | X_1:{n-1})$$$$P(X_1, \\dots, X_n) = \\prod_{k=1}^{n} P(X_k | X_{1:k-1})$$We apply this to words:\n$$P(w_{1:n}) = P(w_1) P(w_2|w_1) P(w_3|w_{1:2}) \\dots P(w_n|w_{1:n-1})$$$$P(w_{1:n}) = \\prod_{k=1}^{n} P(w_k|w_{1:k-1})$$It is still hard to compute $P(w_{1:n})$ using the chain rule. The key insight of the n-gram model is that we can approximate the history just using the last few words. In the case of a bigram model, we approximate $P(w_n | w_{1:n-1})$ by using only the probability of the preceding word $P(w_n|w_{n-1})$\n$$P(w_n|w_{1:n-1}) \\approx P(w_n|w_{n-1})$$ An example of a Markov assumption is when the probability of a word depends only on the previous word in the case of a bigram, a trigram, and a n-gram looks two words, and $n-1$ words into the past, respectively.\nGeneralization:\n$$P(w_n|w_{1:n-1}) \\approx P(w_n|w_{n-N+1:n-1})$$In the case of the bigram, we have:\n$$\\label{bigram_aprox} P(w_{1:n}) = \\prod_{k=1}^{n} P(w_k|w_{1:k-1}) \\approx \\prod_{k=1}^{n} P(w_k|w_{k-1})$$We can estimate equation\\[bigram_aprox\\]{reference-type=“ref” reference=“bigram_aprox”} using the MLE (Maximum likelihood estimation)\n$$P(w_n | w_{n-1}) = \\frac{C(w_{n-1}w_n)}{\\sum_{w} C(w_{n-1} w)}$$ Which can be simplified to\n$$P(w_n | w_{n-1}) = \\frac{C(w_{n-1}w_n)}{C(w_{n-1} )}$$We can generalize the estimation of the MLE n-gram parameter as follows:\n$$P(w_n|w_{n-N+1:n-1}) = \\frac{C(w_{n-N+1:n-1} \\ w_n)}{C(w_{n-N+1:n-1})}$$ This ratio is called the relative frequency\nEvaluating Language models: Training and Test sets There are different ways of evaluating a language model such as extrinsic, and intrinsic evaluation. In extrinsic evaluation we embed the language model i an application and measure how the application’s performance improves, this is a very efficient way of evaluating models, but it is unfortunately very expensive. On the other hand, a intrinsic evaluation metric measures the quality of a model independent of an application, one of this metrics is the perplexity.\nTypes of datasets for model evaluation We need at least three types of datasets for evaluating a language model: training, development, and test sets.\nTraining set: Dataset we use to learn the parameters of our model.\nTest set: Separate dataset from the training set used to evaluate the model. This test should reflect the language we want to use the model for. After training two models in the training set, we can compare how the two trained models fit the test set by determining which model assigns a higher probability to the test set. We only want to test the model once or very few times once on using the test set once the model is ready.\nDevelopment set: We use the development set to do preliminary testing and when we are ready we only use the test set once or very few times.\nEvaluating Language models: Perplexity Perplexity: The perplexity (PP, PPL) of a language model on a test set is the inverse probability of the test set (one over the probability of the test set), normalized by the number of words. This is why sometimes it is called per-word perplexity.\nFor a test set $W=w_1 w_2 \\dots w_N$:\n$$\\begin{aligned} perplexity(W) \u0026= P(w_1 w_2 \\dots w_N)^{-\\frac{1}{N}} \\\\ \u0026 = \\sqrt[N]{\\frac{1}{P(w_1 w_2 \\dots w_N)}} \\end{aligned}$$$$perplexity(W) = \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i|w_1\\dots w_{i-1})}}$$Note that the higher the probability of a word sequence, the lower the perplexity. Thus, the lower the perplexity of a model on the data, the better the model.. Minimizing the perplexity is equivalent to maximizing the test set probability according to the language model.\nThis is how we can calculate the perplexity of a unigram language model:\n$$perplexity(W) = \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i)}}$$And the same for a bigram model:\n$$ \\boxed{ perplexity(W) = \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i|w_{i-1})}} } $$Sampling sentences from a language model One technique to visualize the knowledge of a model is to sample from it. Sampling from a distribution means to choose random points according to their likelihood. Sampling from a language model, which represents a distribution over sentences, means to generate some sentences, choosing each sentence according to its likelihood as defined by the model.\n","wordCount":"748","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-02-26T00:00:00Z","dateModified":"2025-02-26T00:00:00Z","author":{"@type":"Person","name":"Jorge Roldan"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/2_n_gram_language_models/"},"publisher":{"@type":"Organization","name":"Jorge Roldan","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]],tags:"all"},loader:{load:["ui/safe"]}}</script><script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script defer type=text/javascript src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/archives/ title=Archives><span>Archives</span></a></li><li><a href=http://localhost:1313/categories/ title=Categories><span>Categories</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">N-Gram Language Models</h1><div class=post-meta><span title='2025-02-26 00:00:00 +0000 UTC'>February 26, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;748 words&nbsp;·&nbsp;Jorge Roldan</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#n-gram-language-models aria-label="N-gram Language Models">N-gram Language Models</a><ul><li><a href=#n-grams aria-label=N-Grams>N-Grams</a></li><li><a href=#evaluating-language-models-training-and-test-sets aria-label="Evaluating Language models: Training and Test sets">Evaluating Language models: Training and Test sets</a><ul><li><a href=#types-of-datasets-for-model-evaluation aria-label="Types of datasets for model evaluation">Types of datasets for model evaluation</a></li></ul></li><li><a href=#evaluating-language-models-perplexity aria-label="Evaluating Language models: Perplexity">Evaluating Language models: Perplexity</a></li><li><a href=#sampling-sentences-from-a-language-model aria-label="Sampling sentences from a language model">Sampling sentences from a language model</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=n-gram-language-models>N-gram Language Models<a hidden class=anchor aria-hidden=true href=#n-gram-language-models>#</a></h1><h2 id=n-grams>N-Grams<a hidden class=anchor aria-hidden=true href=#n-grams>#</a></h2><p>N-gram models are the simplest type of language models. The N-gram term
has two meanings. One meaning refers to a sequence of n words, so a
2-gram, and 3-gram are sequences of 2, and 3 words, respectively. The
second meaning refers to a probabilistic model that estimates the
probability of a word given the n-1 previous words.</p><p>We represent a sequence of n words as $w_1 \dots w_n$ or $w_{1:n}$ , and
the join probability of each word in a sequence having a value:
$P(W_1 = w_1, X_2=w_2, X_3=w_3, \dots, X_n = w_n$</p><p>To compute the probability of $P(w_1, w_2, \dots, w_n)$ we use the chain
rule of probability</p>$$P(X_1, \dots, X_n) = P(X_1) P(X_2 | X_1) P(X_3 | X_{1:2} \dots P(X_n | X_1:{n-1})$$$$P(X_1, \dots, X_n) = \prod_{k=1}^{n} P(X_k | X_{1:k-1})$$<p>We apply this to words:</p>$$P(w_{1:n}) = P(w_1) P(w_2|w_1) P(w_3|w_{1:2}) \dots P(w_n|w_{1:n-1})$$$$P(w_{1:n}) = \prod_{k=1}^{n} P(w_k|w_{1:k-1})$$<p>It is still hard to compute $P(w_{1:n})$ using the chain rule. The key
insight of the n-gram model is that we can approximate the history just
using the last few words. In the case of a bigram model, we approximate
$P(w_n | w_{1:n-1})$ by using only the probability of the preceding
word $P(w_n|w_{n-1})$</p>$$P(w_n|w_{1:n-1}) \approx P(w_n|w_{n-1})$$<p>An example of a <strong>Markov
assumption</strong> is when the probability of a word depends only on the
previous word in the case of a bigram, a trigram, and a n-gram looks two
words, and $n-1$ words into the past, respectively.</p><p>Generalization:</p>$$P(w_n|w_{1:n-1}) \approx P(w_n|w_{n-N+1:n-1})$$<p>In the case of the bigram, we have:</p>$$\label{bigram_aprox}
P(w_{1:n}) = \prod_{k=1}^{n} P(w_k|w_{1:k-1}) \approx \prod_{k=1}^{n} P(w_k|w_{k-1})$$<p>We can estimate
equation<a href=#bigram_aprox>\[bigram_aprox\]</a>{reference-type=&ldquo;ref&rdquo;
reference=&ldquo;bigram_aprox&rdquo;} using the MLE (Maximum likelihood estimation)</p>$$P(w_n | w_{n-1}) = \frac{C(w_{n-1}w_n)}{\sum_{w} C(w_{n-1} w)}$$<p>Which
can be simplified to</p>$$P(w_n | w_{n-1}) = \frac{C(w_{n-1}w_n)}{C(w_{n-1} )}$$<p>We can generalize the estimation of the MLE n-gram parameter as follows:</p>$$P(w_n|w_{n-N+1:n-1}) = \frac{C(w_{n-N+1:n-1} \ w_n)}{C(w_{n-N+1:n-1})}$$<p>This ratio is called the <strong>relative frequency</strong></p><h2 id=evaluating-language-models-training-and-test-sets class=unnumbered>Evaluating Language models: Training and Test sets<a hidden class=anchor aria-hidden=true href=#evaluating-language-models-training-and-test-sets>#</a></h2><p>There are different ways of evaluating a language model such as
extrinsic, and intrinsic evaluation. In extrinsic evaluation we embed
the language model i an application and measure how the application&rsquo;s
performance improves, this is a very efficient way of evaluating models,
but it is unfortunately very expensive. On the other hand, a intrinsic
evaluation metric measures the quality of a model independent of an
application, one of this metrics is the <strong>perplexity</strong>.</p><h3 id=types-of-datasets-for-model-evaluation class=unnumbered>Types of datasets for model evaluation<a hidden class=anchor aria-hidden=true href=#types-of-datasets-for-model-evaluation>#</a></h3><p>We need at least three types of datasets for evaluating a language
model: training, development, and test sets.</p><p><strong>Training set:</strong> Dataset we use to learn the parameters of our model.</p><p><strong>Test set:</strong> Separate dataset from the training set used to evaluate
the model. This test should reflect the language we want to use the
model for. After training two models in the training set, we can compare
how the two trained models fit the test set by determining which model
assigns a higher probability to the test set. We only want to test the
model once or very few times once on using the test set once the model
is ready.</p><p><strong>Development set:</strong> We use the development set to do preliminary
testing and when we are ready we only use the test set once or very few
times.</p><h2 id=evaluating-language-models-perplexity class=unnumbered>Evaluating Language models: Perplexity<a hidden class=anchor aria-hidden=true href=#evaluating-language-models-perplexity>#</a></h2><p><strong>Perplexity</strong>: The perplexity (PP, PPL) of a language model on a test
set is the inverse probability of the test set (one over the probability
of the test set), normalized by the number of words. This is why
sometimes it is called per-word perplexity.</p><p>For a test set $W=w_1 w_2 \dots w_N$:</p>$$\begin{aligned}
perplexity(W) &= P(w_1 w_2 \dots w_N)^{-\frac{1}{N}} \\
& = \sqrt[N]{\frac{1}{P(w_1 w_2 \dots w_N)}}
\end{aligned}$$$$perplexity(W) = \sqrt[N]{\prod_{i=1}^{N} \frac{1}{P(w_i|w_1\dots w_{i-1})}}$$<p>Note that the higher the probability of a word sequence, the lower the
perplexity. Thus, the <strong>lower the perplexity of a model on the data, the
better the model.</strong>. Minimizing the perplexity is equivalent to
maximizing the test set probability according to the language model.</p><p>This is how we can calculate the perplexity of a unigram language model:</p>$$perplexity(W) = \sqrt[N]{\prod_{i=1}^{N} \frac{1}{P(w_i)}}$$<p>And the same for a bigram model:</p>$$
\boxed{
perplexity(W) = \sqrt[N]{\prod_{i=1}^{N} \frac{1}{P(w_i|w_{i-1})}}
}
$$<h2 id=sampling-sentences-from-a-language-model class=unnumbered>Sampling sentences from a language model<a hidden class=anchor aria-hidden=true href=#sampling-sentences-from-a-language-model>#</a></h2><p>One technique to visualize the knowledge of a model is to sample from
it. Sampling from a distribution means to choose random points according
to their likelihood. Sampling from a language model, which represents a
distribution over sentences, means to generate some sentences, choosing
each sentence according to its likelihood as defined by the model.</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/ngram/>Ngram</a></li><li><a href=http://localhost:1313/tags/language_models/>Language_models</a></li><li><a href=http://localhost:1313/tags/nlp/>Nlp</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/posts/1_setting-up-a-conda-environment/><span class=title>Next »</span><br><span>Setting up a Conda environment</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share N-Gram Language Models on x" href="https://x.com/intent/tweet/?text=N-Gram%20Language%20Models&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2_n_gram_language_models%2f&amp;hashtags=ngram%2clanguage_models%2cnlp"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share N-Gram Language Models on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2_n_gram_language_models%2f&amp;title=N-Gram%20Language%20Models&amp;summary=N-Gram%20Language%20Models&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2f2_n_gram_language_models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share N-Gram Language Models on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2_n_gram_language_models%2f&title=N-Gram%20Language%20Models"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share N-Gram Language Models on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2f2_n_gram_language_models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share N-Gram Language Models on whatsapp" href="https://api.whatsapp.com/send?text=N-Gram%20Language%20Models%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2f2_n_gram_language_models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share N-Gram Language Models on telegram" href="https://telegram.me/share/url?text=N-Gram%20Language%20Models&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2_n_gram_language_models%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share N-Gram Language Models on ycombinator" href="https://news.ycombinator.com/submitlink?t=N-Gram%20Language%20Models&u=http%3a%2f%2flocalhost%3a1313%2fposts%2f2_n_gram_language_models%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Jorge Roldan</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>